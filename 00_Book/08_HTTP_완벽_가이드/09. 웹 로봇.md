# 9. 웹 로봇

## 9.1 크롤러와 크롤링

### 9.1.1 어디에서 시작하는가: '루트 집합'

- **루트 집합** : 크롤러가 방문을 시작하는 URL들의 초기 집합
- 루트 집합을 고를 때, 관심 있는 웹페이지들의 대부분을 가져오게 될 수 있도록 충분히 다른 장소에서 URL들을 선택

### 9.1.2 링크 추출과 상대 링크 정상화

- 크롤러는 웹을 돌아다니면서 꾸준히 HTML 문서를 검색
- 검색한 각 페이지 안에 들어있는 URL 링크들을 파싱해서 크롤링할 페이지들의 목록에 추가해야 함
- 크롤링을 진행하면서 탐색해야 할 새 링크를 발견함에 따라, 이 목록은 보통 급속히 확장
- 크롤러들은 간단한 HTML 파싱을 해서 이들 링크들을 추출하고 상대 링크를 절대 링크로 변환할 필요 존재

### 9.1.3 순환 피하기

- 로봇이 웹을 크롤링 할 때, 루프나 순환에 빠지지 않도록 매우 조심해야 함
- 순환을 피하기 위해 반드시 그들이 어디를 방문했는지 알아야 함
- 순환은 로봇을 함정에 빠뜨려서 멈추게 하거나 진행을 느려지게 함

### 9.1.4 루프와 중복

- 같은 페이지들을 반복해서 가져오는데 모든 시간을 허비하게 만들 수 있음
- 크롤러가 네트워크 대역폭을 다 차지하면 어떤 페이지도 가져올 수 없게 될 수 있음
- 크롤러가 같은 페이지를 반복해서 가져오면 고스란히 웹 서버의 부담이 됨
- 크롤러의 네트워크 접속 속도가 충분히 빠르다면, 웹사이트를 압박하여 어떤 사용자도 접근할 수 없게 만들 수 있음(법적 문제)

### 9.1.5 빵 부스러기의 흔적

- 방문한 곳을 지속적으로 추적하는 것은 쉽지 않음
- URL들은 굉장히 많기 때문에, 어떤 URL을 방문했는지 속도와 메모리 사용 면에서 효과적인 자료 구조를 사용할 필요가 있음
  - 트리와 해시 테이블
  - 느슨한 존재 비트맵
  - 체크포인트
  - 파티셔닝

### 9.1.6 별칭과 로봇 순환

- 한 URL이 다른 URL에 대한 별칭이라면, 그 둘이 서로 달라 보이더라도 사실은 같은 리소스를 가리키고 있음

### 9.1.7 URL 정규화하기

- 로봇은 포트 번호가 명시되지 않았다면, `:80`을 추가하거나, 이스케이핑 문자를 대응되는 문자로 변환하거나, `#` 태그 제거
- URL 정규화는 기본적인 문법의 별칭을 제거 가능하지만, 예외사항이 존재할 것

## 9.2 로봇의 HTTP

- 많은 로봇이 HTTP/1.0은 요구사항이 적기 때문에 많은 로봇이 이용

### 9.2.1 요청 헤더 식별하기

- 로봇 대부분은 약간의 식원 식별 헤더(User-Agent)를 구현하고 전송

- 로봇 구현자들은 기본적인 몇 가지 헤더를 사이트에 보내주는 것이 좋음
  - `User-Agent` : 로봇의 이름
  - `From` : 로봇의 사용자/관리자의 이메일 주소 제공
  - `Accept` : 서버에게 어떤 미디어 타입을 보내도 되는지 말해줌
  - `Referrer` : 현재의 요청 URL을 포함한 문서의 URL 제공

### 9.2.2 가상 호스팅

- `Host` 헤더 지원 필요
  - `Host` 헤더를 포함하지 않으면 로봇이 어떤 URL에 대해 잘못된 컨텐츠를 찾게 만듦
  - 이러한 이유로 HTTP/1.1은 `Host` 헤더를 사용할 것을 요구
- `Host` 헤더를 포함하지 않은 크롤러
  - 두 개의 사이트를 운영하는 서버를 크롤링하는 경우 엉뚱한 컨텐츠 획득 가능

### 9.2.3 조건부 요청

- 로봇은 때때로 극악한 양의 요청을 시도할 수 있는데, 로봇이 검색하는 컨텐츠의 양을 최소화하는 것의 의미 있음

- 앤터티 태그를 비교함으로써 그들이 받아간 마지막 버전 이후 최신임을 확인하는 조건부 HTTP 요청을 구현

### 9.2.4 응답 다루기

- 대다수의 로봇들은 주 관심사가 단순히 GET 메서드로 컨텐츠를 요청해서 가져옴
- HTTP의 몇몇 기능(조건부 요청 등)을 사용하는 로봇이나, 웹 탐색이나 서버와의 상호작용을 더 잘 해보려고하는 로봇들은 여러 종류의 HTTP 응답을 다룰 줄  알 필요가 있음
  - 상태 코드
    - 최소한 일반적인 상태 코드나 예상 가능한 상태 코드를 다룰 수 있어야 함(200, 404 등)
    - 모든 서버가 언제나 적절한 에러 코드를 반환하지는 않는다는 걸 알아두는 것이 중요
  - 앤터티
    - HTTP 헤더에 임베딩된 정보를 따라 로봇들은 엔터티 자체의 정보 찾기 가능
    - 메타 `http-equiv` 태과와 같은 메타 HTML 태그

### 9.2.5 User-Agent 타게팅

- 웹 관리자는 많은 로봇이 사이트에 방문하게 될 것임을 명심하고, 그 로봇들로부터의 요청을 예상해야 함
- 여러 브라우저를 지원 가능하도록 개발

## 9.3 부적절하게 동작하는 로봇들

- 제멋대로인 로봇들이 아수라장을 만들 여러 가능성 존재
- 로봇들이 저지르는 실수 몇 가지와 그로 인해 초래되는 결과
  - **폭주하는 로봇**
    - 로봇은 웹 서핑을 하는 사람보다 훨씬 빠르게 HTTP 요청을 만들 수 있음
    - 로봇이 논리적인 에러를 갖고 있거나 순환에 빠졌다면 웹 서버에 극심한 부하를 줄 수 있음
    - **로봇 개발자들은 폭주 방지를 위한 보호 장치를 반드시 극히 신경 써서 설계**
  - **오래된 URL**
    - 몇몇 로봇은 URL 목록을 방문하는데, 목록이 오래되었을 가능성이 존재
    - 존재하지 않는 URL에 대한 요청을 많이 보낸다면 에러 로그가 채워지거나 에러 페이지 제공 부하를 줄 수 있음
  - **길고 잘못된 URL**
    - 순환이나 프로그래밍상의 오류로 로봇은 웹 사이트에 크고 의미 없는 URL 요청 가능
    - URL이 길다면 웹 서버의 처리 능력에 영향을 주고, 접근 로그를 어지럽게 채우고, 허술하다면 고장을 일으킬 수 있음
  - **호기심이 지나친 로봇**
  - **동적 게이트웨이 접근**
    - 로봇이 접근하고 있는 것에 대해 언제나 잘 알고 있는 것은 아님
    - 로봇은 게이트웨이 애플리케이션의 컨텐츠에 대한 URL로 요청 가능
    - 이 경우에 얻는 데이터는 특수 목적을 위한 것일테고 처리 비용이 많이 들 것
    - 관리자는 게이트웨이에서 얻은 문서를 요청하는 로봇들을 좋아하지 않음

## 9.4 로봇 차단하기

- 문서 루트에 `robots.txt`라고 이름 붙은 선택적 파일 제공
- 로봇이 이 표준에 따른다면 리소스에 접근하기 전에 우선 `robots.txt`를 요청

### 9.4.1 로봇 차단 표준

- 로봇 차단 표준은 임시방편으로 마련된 표준