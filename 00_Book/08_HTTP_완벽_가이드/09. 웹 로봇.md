# 9. 웹 로봇

## 9.1 크롤러와 크롤링

### 9.1.1 어디에서 시작하는가: '루트 집합'

- **루트 집합** : 크롤러가 방문을 시작하는 URL들의 초기 집합
- 루트 집합을 고를 때, 관심 있는 웹페이지들의 대부분을 가져오게 될 수 있도록 충분히 다른 장소에서 URL들을 선택

### 9.1.2 링크 추출과 상대 링크 정상화

- 크롤러는 웹을 돌아다니면서 꾸준히 HTML 문서를 검색
- 검색한 각 페이지 안에 들어있는 URL 링크들을 파싱해서 크롤링할 페이지들의 목록에 추가해야 함
- 크롤링을 진행하면서 탐색해야 할 새 링크를 발견함에 따라, 이 목록은 보통 급속히 확장
- 크롤러들은 간단한 HTML 파싱을 해서 이들 링크들을 추출하고 상대 링크를 절대 링크로 변환할 필요 존재

### 9.1.3 순환 피하기

- 로봇이 웹을 크롤링 할 때, 루프나 순환에 빠지지 않도록 매우 조심해야 함
- 순환을 피하기 위해 반드시 그들이 어디를 방문했는지 알아야 함
- 순환은 로봇을 함정에 빠뜨려서 멈추게 하거나 진행을 느려지게 함

### 9.1.4 루프와 중복

- 같은 페이지들을 반복해서 가져오는데 모든 시간을 허비하게 만들 수 있음
- 크롤러가 네트워크 대역폭을 다 차지하면 어떤 페이지도 가져올 수 없게 될 수 있음
- 크롤러가 같은 페이지를 반복해서 가져오면 고스란히 웹 서버의 부담이 됨
- 크롤러의 네트워크 접속 속도가 충분히 빠르다면, 웹사이트를 압박하여 어떤 사용자도 접근할 수 없게 만들 수 있음(법적 문제)

### 9.1.5 빵 부스러기의 흔적

- 방문한 곳을 지속적으로 추적하는 것은 쉽지 않음
- URL들은 굉장히 많기 때문에, 어떤 URL을 방문했는지 속도와 메모리 사용 면에서 효과적인 자료 구조를 사용할 필요가 있음
  - 트리와 해시 테이블
  - 느슨한 존재 비트맵
  - 체크포인트
  - 파티셔닝

### 9.1.6 별칭과 로봇 순환

- 한 URL이 다른 URL에 대한 별칭이라면, 그 둘이 서로 달라 보이더라도 사실은 같은 리소스를 가리키고 있음

### 9.1.7 URL 정규화하기

- 로봇은 포트 번호가 명시되지 않았다면, `:80`을 추가하거나, 이스케이핑 문자를 대응되는 문자로 변환하거나, `#` 태그 제거
- URL 정규화는 기본적인 문법의 별칭을 제거 가능하지만, 예외사항이 존재할 것

